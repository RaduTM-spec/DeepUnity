RWStructuredBuffer<float> input; // (in_channels, in_height, in_width)
RWStructuredBuffer<float> gamma; // (out_channels, in_channels, kernelSize, kernelSize)
RWStructuredBuffer<float> beta; // (out_channels)
RWStructuredBuffer<float> output;
RWStructuredBuffer<float> gamma_grad;
RWStructuredBuffer<float> beta_grad;
RWStructuredBuffer<float> input_grad;
RWStructuredBuffer<float> loss;

int batch_size;
int in_channels;
int in_height;
int in_width;
int out_channels;
int out_height;
int out_width;
int kernel_height;
int kernel_width;
int grad_scale;


float gamma_get(int b, int c, int h, int w)
{
    return gamma[b * in_channels * kernel_height * kernel_width + c * kernel_height * kernel_width + h * kernel_width + w];
}
void gamma_grad_add(int b, int c, int h, int w, float value)
{
    gamma_grad[b * in_channels * kernel_height * kernel_width + c * kernel_height * kernel_width + h * kernel_width + w] += value;
}
float input_get(int b, int c, int h, int w)
{
    return input[b * in_channels * in_height * in_width + c * in_height * in_width + h * in_width + w];
}
void output_add(int b, int c, int h, int w, float value)
{
    output[b * out_channels * out_height * out_width + c * out_height * out_width + h * out_width + w] += value;
}
float loss_get(int b, int c, int h, int w)
{
    return loss[b * out_channels * out_height * out_width + c * out_height * out_width + h * out_width + w];
}
void input_grad_add(int b, int c, int h, int w, float value)
{
    input_grad[b * in_channels * in_height * in_width + c * in_height * in_width + h * in_width + w] += value;
}

#pragma kernel Forward
[numthreads(16, 16, 4)]
void Forward(int3 id : SV_DispatchThreadID)
{
   // id.x = out_width, id.y = out_height, id.z = out_channels
    
    if (id.x >= out_width || id.y >= out_height || id.z >= out_channels)
        return;
    
    for (int b = 0; b < batch_size; b++)
    {
        for (int ic = 0; ic < in_channels; ic++)
        {
            float sum = beta[id.z] / in_channels; 
            for (int j = 0; j < kernel_height; j++)
            {
                for (int i = 0; i < kernel_width; i++)
                {
                    sum += input_get(b, ic, id.y + j, id.x + i) * gamma_get(id.z, ic, j, i);
                }
            }
            output_add(b, id.z, id.y, id.x, sum);
        }
    }
}


#pragma kernel ComputeKernelsGradients3
[numthreads(3, 3, 64)]
void ComputeKernelsGradients3(int3 id : SV_DispatchThreadID)
{
    // id.x = kernel_width, id.y = kernel_height, id.z = out_channels
     // Performs 2d correlation between the input and loss to compute the gradients for kernels;
    
    if (id.x >= kernel_width || id.y >= kernel_height || id.z >= out_channels)
        return;
    
    for (int b = 0; b < batch_size; b++)
    {
        for (int ic = 0; ic < in_channels; ic++)
        {
            float sum = 0.;
            
            for (int j = 0; j < out_height; j++)
            {
                for (int i = 0; i < out_width; i++)
                {
                    sum += input_get(b, ic, j + id.y, i + id.x) * loss_get(b, id.z, j, i);
                }
            }
           
            gamma_grad_add(id.z, ic, id.y, id.x, sum / grad_scale);
        }
    }
    
    // Beta grad computing on GPU was deprecated due the fact is faster on CPU
}

#pragma kernel ComputeKernelsGradients5
[numthreads(5, 5, 32)]
void ComputeKernelsGradients5(int3 id : SV_DispatchThreadID)
{
    // id.x = kernel_width, id.y = kernel_height, id.z = out_channels
     // Performs 2d correlation between the input and loss to compute the gradients for kernels;
    
    if (id.x >= kernel_width || id.y >= kernel_height || id.z >= out_channels)
        return;
    
    for (int b = 0; b < batch_size; b++)
    {
        for (int ic = 0; ic < in_channels; ic++)
        {
            float sum = 0.;
            
            for (int j = 0; j < out_height; j++)
            {
                for (int i = 0; i < out_width; i++)
                {
                    sum += input_get(b, ic, id.y + j, id.x + i) * loss_get(b, id.z, j, i);
                }
            }
           
            gamma_grad_add(id.z, ic, id.y, id.x, sum / grad_scale);
        }
    }
    
    // Beta grad computing on GPU was deprecated due the fact is faster on CPU
}

#pragma kernel ComputeInputGradient
[numthreads(16, 16, 4)]
void ComputeInputGradient(int3 id : SV_DispatchThreadID)
{
    // id.x = in_width, id.y = in_height, id.z = batch_size
    // Convolves loss and kernels
    
    if (id.x >= in_width || id.y >= in_height || id.z == batch_size)
        return;
    
    for (int ic = 0; ic < in_channels; ic++)
    {
        for (int oc = 0; oc < out_channels; oc++)
        {
            float sum = 0.f;
            for (int j = 0; j < kernel_height; j++)
            {
                for (int i = 0; i < kernel_width; i++)
                {
                    int inputRow = id.y - j;
                    int inputCol = id.x - i;
                    
                    if (inputRow >= 0 && inputRow < out_height && inputCol >= 0 && inputCol < out_width)
                    {
                        // the kernels are rotated by 180 degrees, so we get the inversed index of the kernel.
                        int jIndexOfRotatedKernel = kernel_height - j - 1;
                        int iIndexOfRotatedKernel = kernel_width - i - 1;
                        sum += loss_get(id.z, oc, inputRow, inputCol) * gamma_get(oc, ic, jIndexOfRotatedKernel, iIndexOfRotatedKernel);
                    }
                }

            }
            
            input_grad_add(id.z, ic, id.y, id.x, sum / grad_scale);
        }

    }

}
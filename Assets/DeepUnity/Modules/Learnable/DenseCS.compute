// This script is used to resolve all Dense calculations but on GPU
RWStructuredBuffer<float> transposed_gamma; //tranposed
RWStructuredBuffer<float> gamma;
RWStructuredBuffer<float> gamma_grad;
RWStructuredBuffer<float> beta;
RWStructuredBuffer<float> beta_grad;
RWStructuredBuffer<float> input;
RWStructuredBuffer<float> output;
RWStructuredBuffer<float> loss;
RWStructuredBuffer<float> transposed_loss;

int batch_size;
int in_features;
int out_features;
int gamma_rank;
int input_rank; // loss_rank

float transposed_gamma_get(int h, int w)
{
    return transposed_gamma[h * out_features + w];
}
float gamma_get(int h, int w)
{
    return gamma[h * in_features + w];
}
void gamma_grad_add(int h, int w, float val)
{
    gamma_grad[h * in_features + w] += val;
}
float beta_get(int w)
{
    return beta[w];
}
void beta_grad_add(int w, int val)
{
    beta_grad[w] += val;
}
float input_get(int h, int w)
{
    return input[h * in_features + w];
}
void output_set(int h, int w, float val)
{
    output[h * out_features + w] = val;
}
float transposed_loss_get(int h, int w)
{
    return transposed_loss[h * batch_size + w];   
}

#pragma kernel Forward
[numthreads(32,32,1)]
void Forward (int3 id : SV_DispatchThreadID)
{
    // id.x = out_features, id.y = batch_size
    // id.x = p , id.y = n
    
    // input [batch_size * in_features] * tranposed_gamma[in_features, out_features]
    // output [batch_size * out_features]
    
    // input [J x 1 x N x M] * other [K x M x P]
    // out [J x K x N x P]
    
    // make sure if id is inside the matrix
    if (id.x >= out_features || id.y >= batch_size)
        return;
    
    if (gamma_rank == 1)
    {
        float sum = beta_get(id.x);
        for (int m = 0; m < in_features; m++)
        {
            float l = input_get(id.y, m);
            float r = transposed_gamma_get(0, m);
            sum += l * r;
        }
        
        output_set(0, id.y, sum);      
    }
    else if (input_rank == 1)
    {
        float sum = beta_get(id.x);
        for (int m = 0; m < in_features; m++)
        {
            float l = input_get(0, m);
            float r = transposed_gamma_get(m, id.x);
            sum += l * r;
        }
        
        output_set(0, id.x, sum);       
    }
    else
    {
        float sum = beta_get(id.x);
        for (int m = 0; m < in_features; m++)
        {
            sum += input_get(id.y, m) * transposed_gamma_get(m, id.x);
        }
        
        output_set(id.y, id.x, sum);        
    }    
}

#pragma kernel ComputeGradients
[numthreads(32, 32, 1)]
void ComputeGradients(int3 id : SV_DispatchThreadID)
{
    // perform 2 matmuls
     
    // MatMul(transposedLoss, InputCache);
    // id.x = in_features, id.y = out_features   
    // transposed_loss [out_features, batch_size] * input[batch_size, in_features]
    // gamma_grad [out_features, in_features]
    int loss_rank = input_rank;
    
    if(id.x < in_features && id.y < out_features)
    {
        // if (input_rank == 1)
        // {
        //     float sum = 0.f;
        //     for (int m = 0; m < batch_size; m++)
        //     {
        //         float l = transposed_loss_get(id.y, m);
        //         float r = input_get(0, m);
        //         sum += l * r;
        //     }
        // 
        //     gamma_grad_add(0, id.y, sum / batch_size);
        // }
        // else if (loss_rank == 1)
        // {
        //     float sum = 0.f;
        //     for (int m = 0; m < batch_size; m++)
        //     {
        //         float l = transposed_loss_get(0, m);
        //         float r = input_get(m, id.x);
        //         sum += l * r;
        //     }
        // 
        //     gamma_grad_add(0, id.x, sum / batch_size);
        // }
        // else
        // {
            float sum = 0.f;
            for (int m = 0; m < batch_size; m++)
            {
                sum += transposed_loss_get(id.y, m) * input_get(m, id.x);
            }
        
            gamma_grad_add(id.y, id.x, sum / batch_size);
        // }
    }
    
    
    // MatMul(transposedLoss, Tensor.Ones(batch_size));
    // id.x = 1, id.y = out_features   
    // transposed_loss [out_features, batch_size] * ones[batch_size]
    // beta_grad [out_features]
    
    if(id.x < 1 && id.y < out_features)
    {
        float sum = 0.f;
        for (int m = 0; m < batch_size; m++)
        {
            float l = transposed_loss_get(id.y, m);
            float r = 1;
            sum += l * r;
        }      
        beta_grad_add(id.y, sum / batch_size);
    }
}

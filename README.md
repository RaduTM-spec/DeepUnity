# DeepUnity
###### In development - does not currently accept Pull Requests, though feel free to Fork and expand upon it
![version](https://img.shields.io/badge/version-v0.9.6.12-blue)

DeepUnity is an add-on framework that provides tensor computation [with GPU acceleration support] and deep neural networks, along with reinforcement learning tools that enable training for intelligent agents within Unity environments using Proximal Policy Optimization (PPO) and Soft Actor-Critic (SAC).

#### Run your first DeepUnity script
```csharp
using UnityEngine;
using DeepUnity;

namespace DeepUnityTutorials
{
    public class Tutorial : MonoBehaviour
    {
        [Header("Learning z = x^2 + y^2.")]
        public NeuralNetwork network;
        private Optimizer optim;
        private LRScheduler scheduler;

        private Tensor train_inputs;
        private Tensor train_targets;
        private Tensor valid_inputs;
        private Tensor valid_targets;

        public void Start()
        {
            if (network == null)
            {
                network = new NeuralNetwork(
                    new Dense(2, 64),
                    new Tanh(),
                    new Dense(64, 64, device: Device.GPU),
                    new ReLU(),
                    new Dense(64, 1)).CreateAsset("TutorialModel");
            }
            optim = new Adam(network.Parameters());
            scheduler = new LRScheduler(optim, 30, 0.1f);

            // Generate training dataset
            int data_size = 1024;
            Tensor x = Tensor.RandomNormal(data_size, 1);
            Tensor y = Tensor.RandomNormal(data_size, 1);
            train_inputs = Tensor.Cat(1, x, y);
            train_targets = x * x + y * y;

            // Generate validation set
            int valid_size = 64;
            x = Tensor.RandomNormal(valid_size, 1);
            y = Tensor.RandomNormal(valid_size, 1);
            valid_inputs = Tensor.Cat(1, x, y);
            valid_targets = x * x + y * y;
        }

        public void Update()
        {
            // Training. Split the dataset into batches of 32.
            float train_loss = 0f;
            Tensor[] input_batches = train_inputs.Split(0, 32);
            Tensor[] target_batches = train_targets.Split(0, 32);
            for (int i = 0; i < input_batches.Length; i++)
            {
                Tensor prediction = network.Forward(input_batches[i]);
                Loss loss = Loss.MSE(prediction, target_batches[i]);

                optim.ZeroGrad();
                network.Backward(loss.Gradient);
                optim.ClipGradNorm(0.5f);
                optim.Step();

                train_loss += loss.Item;
            }
            train_loss /= input_batches.Length;
            
            // Validation
            Tensor valid_prediction = network.Predict(valid_inputs);
            float valid_loss = Metrics.MeanSquaredError(valid_prediction, valid_targets);
            print($"Epoch: {Time.frameCount} - Train Loss: {train_loss} - Valid Loss: {valid_loss}");
            
            scheduler.Step();
            network.Save();
        }
    }
}
```
###### _Digits generated by a Generative Adversarial Network (GAN) trained on MNIST dataset._
![digits](https://github.com/smtmRadu/DeepUnity/blob/main/Assets/DeepUnity/Documentation/gan.gif?raw=true)

###### _Digits reconstructed by a Variational Auto-Encoder (VAE) trained on MNIST dataset. (original - first line, reconstructed - second line)_
![digits](https://github.com/smtmRadu/DeepUnity/blob/main/Assets/DeepUnity/Documentation/vae.gif?raw=true)

## Reinforcement Learning
In order to work with Reinforcement Learning tools, you must create a 2D or 3D agent using Unity provided GameObjects and Components. The setup flow works similary to ML Agents, so you must create a new behaviour script (e.g. _ReachGoal_) that must inherit the **Agent** class. Attach the new behaviour script to the agent GameObject (automatically **DecisionRequester** script is attached too) [Optionally, a **TrainingStatistics** script can be attached]. Choose the space size and number of continuous/discrete actions, then override the following methods in the behavior script:
- _CollectObservations()_
- _OnActionReceived()_
- _Heuristic()_ [Optional]
- _OnEpisodeBegin()_ [Optional]

Also in order to decide the reward function and episode's terminal state, use the following calls:
-  _AddReward(*reward*)_
-  _SetReward(*reward*)_
-  _EndEpsiode()_ 

When the setup is ready, press the _Bake_ button; a behaviour along with all neural networks and hyperparameters assets are created inside a folder with the _behaviour's name_, located in _Assets/_ folder. From this point everything is ready to go. 

To get into advanced training, check out the following assets created:
- **Behaviour** can be set whether to use a fixed or trainable standard deviation for continuous actions. Inference and Training devices are also available to be set (set both on CPU if your machine lacks a graphics card). TargetFPS modifies the rate of physics update, being equal to _1 / Time.fixedDeltaTime (default: 50)_.
- **Config** provides all hyperparameters necesarry for a custom training session.

#### Behaviour script overriding example
```csharp
using UnityEngine;
using DeepUnity;

public class MoveToGoal : Agent
{
    [Header("Properties")]
    public Transform apple;
    public float speed = 10f;
    public float norm_scale = 8f;
    public override void OnEpisodeBegin()
    {
        float xrand = Random.Range(-norm_scale, norm_scale);
        float zrand = Random.Range(-norm_scale, norm_scale);
        apple.localPosition = new Vector3(xrand, 2.25f, zrand);
        
        xrand = Random.Range(-norm_scale, norm_scale);
        zrand = Random.Range(-norm_scale, norm_scale);
        transform.localPosition = new Vector3(xrand, 2.25f, zrand);
    }
    public override void CollectObservations(StateVector sensorBuffer)
    {
        sensorBuffer.AddObservation(transform.localPosition.x / norm_scale);
        sensorBuffer.AddObservation(transform.localPosition.z / norm_scale);
        sensorBuffer.AddObservation(apple.transform.localPosition.x / norm_scale);
        sensorBuffer.AddObservation(apple.transform.localPosition.z / norm_scale);
    }
    public override void OnActionReceived(ActionBuffer actionBuffer)
    {
        float xmov = actionBuffer.ContinuousActions[0];
        float zmov = actionBuffer.ContinuousActions[1];

        transform.position += new Vector3(xmov, 0, zmov) * Time.fixedDeltaTime * speed;
        AddReward(-0.0025f); // Step penalty
    }
    public override void Heuristic(ActionBuffer actionsOut)
    {
        float xmov = 0;
        float zmov = 0;

        if (Input.GetKey(KeyCode.W))
            zmov = 1;
        else if (Input.GetKey(KeyCode.S))
            zmov = -1;

        if (Input.GetKey(KeyCode.D))
            xmov = 1;
        else if (Input.GetKey(KeyCode.A))
            xmov = -1;

        actionsOut.ContinuousActions[0] = xmov;
        actionsOut.ContinuousActions[1] = zmov;
        
    }  

    private void OnTriggerEnter(Collider other)
    {
        if (other.CompareTag("Apple"))
        {
            SetReward(1f);
            EndEpisode();
        }
        if (other.CompareTag("Wall"))
        {
            SetReward(-1f);
            EndEpisode();
        }
    }
}
```
_This example considers an agent (with 4 space size and 2 continuous actions) positioned in the middle of an arena that moves forward, backward, left or right (Decision Period is 1), and must reach a randomly positioned goal (see GIF below). The agent is rewarded by 1 point if he touches the apple, and penalized by 1 point if he is hitting a wall (or falls of the floor), and on every collision the episode ends._

![reacher](https://github.com/smtmRadu/DeepUnity/blob/main/Assets/DeepUnity/Documentation/reacher.gif?raw=true)



TIPS: 
- **Parallel training** is one option to use your device at maximum efficiency. After inserting your agent inside an Environment GameObject, you can duplicate that environment several times along the scene before starting the training session; this method is necessary for multi-agent co-op or adversarial training. Note that DeepUnity dynamically adapts the timescale of the simulation to get the maximum efficiency out of your machine.

- In order to properly get use of _AddReward()_ and _EndEpisode()_ consult the diagram below. These methods work well being called inside _OnTriggerXXX()_ or _OnCollisionXXX()_, as well as inside _OnActionReceived()_ rightafter actions are performed. 

- **Decision Period** high values increases overall performance of the training session, but lacks when it comes to agent inference accuracy. Typically, use a higher value for broader parallel environments, then decrease this value to 1 to fine-tune the agent.

- **Input Normalization** plays a huge role in policy convergence. To outcome this problem, observations can be auto-normalized by checking the corresponding box inside behaviour asset, but instead, is highly recommended to manually normalize all input values before adding them to the __SensorBuffer__. Scalar values can be normalized within [0, 1] or [-1, 1] ranges by using the formula **normalized_value = (value - min) / (max - min)**. Note that inputs are clamped by default in [-3, 3] for network stability.

- The following MonoBehaviour methods: **Awake()**, **Start()**, **FixedUpdate()**, **Update()** and **LateUpdate()** are virtual. If neccesary, in order to override them, call the their **base** each time, respecting the logic of the diagram below.

###### _Base Agent class_
![agentclass](https://github.com/smtmRadu/DeepUnity/blob/main/Assets/DeepUnity/Documentation/agentclass.jpg?raw=true)

All tutorials scripts are included inside _Assets/DeepUnity/Tutorials_ folder, containing all features provided by the framework and RL environments inspired from ML-Agents examples (note that not all of them have trained models attached).

###### _Sorter agent whose task is to visit the tiles in ascending order_
![sorter](https://github.com/smtmRadu/DeepUnity/blob/main/Assets/DeepUnity/Documentation/sorter.gif?raw=true)


_A paper describing how to implement deep neural nets, PPO and SAC from scratch will be released this spring..._

![rl](https://github.com/smtmRadu/DeepUnity/blob/main/Assets/DeepUnity/Documentation/tensors.png?raw=true)


